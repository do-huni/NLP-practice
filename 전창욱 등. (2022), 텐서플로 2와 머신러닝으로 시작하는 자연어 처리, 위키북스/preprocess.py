{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUwq4xUo+NJBzkK0BZeKrP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/do-huni/NLP_practice/blob/main/%EC%A0%84%EC%B0%BD%EC%9A%B1%20%EB%93%B1.%20(2022)%2C%20%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%202%EC%99%80%20%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%9C%BC%EB%A1%9C%20%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%20%EC%9E%90%EC%97%B0%EC%96%B4%20%EC%B2%98%EB%A6%AC%2C%20%EC%9C%84%ED%82%A4%EB%B6%81%EC%8A%A4/preprocess.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "TUPoS01hQVTb",
        "outputId": "8ac2ad4e-1bcf-43d2-ea94-7066a0cdf3ef"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a765d7815e3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#NAMESPACE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "#NAMESPACE\n",
        "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "\n",
        "#tokens\n",
        "PAD = \"<PAD>\"\n",
        "STD = \"<SOS>\" #Start of Sequence\n",
        "END = \"<END>\" #End of Sequence\n",
        "UNK = \"<UNK>\" #Unknown\n",
        "\n",
        "#indexes\n",
        "PAD_INDEX = 0\n",
        "STD_INDEX = 1\n",
        "END_INDEX = 2\n",
        "UNK_INDEX = 3\n",
        "\n",
        "MARKER = [PAD, STD, END, UNK]\n",
        "CHANGE_FILTER = re.compile(FILTERS)\n",
        "\n",
        "MAX_SEQUENCE = 25\n",
        "\n",
        "def load_data(path):\n",
        "  data_df = pd.read_csv(path, header = 0)\n",
        "  question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "\n",
        "  return question, answer\n",
        "\n",
        "def data_tokenizer(data):\n",
        "  words = [] #word_dictionary\n",
        "  for sentence in data:\n",
        "    sentence = re.sub(CHANGE_FILTER, '', sentence) #Removing Special Symbols\n",
        "    for word in sentence.split():\n",
        "      words.append(word)\n",
        "  return [word for word in words if word] #exclude empty element\n",
        "\n",
        "def morpheme_tokenizer(data):\n",
        "  morph_analyzer = Okt()\n",
        "  result_data = []\n",
        "  for seq in tqdm(data):\n",
        "    morpheme_seq = ' '.join(morph_analyzer.morphs(re.sub(' ', '', seq)))\n",
        "    result_data.append(morpheme_seq)\n",
        "\n",
        "  return result_data\n",
        "\n",
        "def load_word_dictionary(path, vocab_path, tokenize_into_morphemes = False):\n",
        "  word_dictionary = []\n",
        "  if not os.path.exists(vocab_path):\n",
        "    if os.path.exists(path):\n",
        "      data_df = pd.read_csv(path, encoding = 'utf-8')\n",
        "      question, answer = list(data_df['Q']), list(data_df[\"A\"])\n",
        "      if tokenize_into_morphemes:\n",
        "        question = morpheme_tokenizer(question)\n",
        "        answer = morpheme_tokenizer(answer)\n",
        "      \n",
        "      data = []\n",
        "      data.extend(question)\n",
        "      data.extend(answer)\n",
        "      words= data_tokenizer(data)\n",
        "      words = list(set(words)) #excluding overlapped words\n",
        "      words[:0] = MARKER\n",
        "    \n",
        "    with open(vocab_path, 'w', encoding = 'utf-8') as vocabulary_file:\n",
        "      for word in words:\n",
        "        vocabulary_file.write(word + '\\n')\n",
        "    \n",
        "  with open(vocab_path, 'r', encoding = 'utf-8') as vocabulary_file:\n",
        "    for line in vocabulary_file:\n",
        "      word_dictionary.append(line.strip())\n",
        "  \n",
        "  word2idx, idx2word = make_vocabulary(word_dictionary)\n",
        "\n",
        "  return word2idx, idx2word, len(word2idx)\n",
        "\n",
        "def make_vocabulary(word_dictionary):\n",
        "  word2idx = {word: idx for idx, word in enumerate(word_dictionary)}\n",
        "  idx2word = {idx: word for idx, word in enumerate(word_dictionary)}\n",
        "  \n",
        "  return word2idx, idx2word\n",
        "\n",
        "def enc_preprocessing(value, dictionary, tokenize_into_morphemes = False):\n",
        "  sequences_input_index = []\n",
        "  sequences_length = []\n",
        "\n",
        "  if tokenize_into_morphemes:\n",
        "    value = morpheme_tokenizer(value)\n",
        "  \n",
        "  for sequence in value:\n",
        "    sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "    sequence_index = []\n",
        "    for word in sequence.split():\n",
        "      if dictionary.get(word) is not None:\n",
        "        sequence_index.extend([dictionary[word]])\n",
        "      else:\n",
        "        sequence_index.extend([dictionary[UNK]])\n",
        "    \n",
        "    if len(sequence_index) > MAX_SEQUENCE:\n",
        "      sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "  \n",
        "    sequences_length.append(len(sequence_index))\n",
        "    sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "\n",
        "    sequences_input_index.append(sequence_index)\n",
        "  \n",
        "  return np.asarray(sequences_input_index), sequences_length\n",
        "\n",
        "def dec_output_preprocessing(value, dictionary, tokenize_into_morphemes = False):\n",
        "  sequences_output_index = []\n",
        "  sequences_length = []\n",
        "\n",
        "  if tokenize_into_morphemes:\n",
        "    value = morpheme_tokenizer(value)\n",
        "  \n",
        "  for sequence in value:\n",
        "    sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "    sequence_index = []\n",
        "    sequence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "\n",
        "    if len(sequence_index) > MAX_SEQUENCE:\n",
        "      sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "\n",
        "    sequences_length.append(len(sequence_index))\n",
        "    sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "\n",
        "    sequences_output_index.append(sequence_index)\n",
        "  \n",
        "  return np.asarray(sequences_output_index), sequences_length\n",
        "\n",
        "def dec_target_preprocessing(value, dictionary, tokenize_into_morphemes = False):\n",
        "  sequences_target_index = []\n",
        "\n",
        "  if tokenize_into_morphemes:\n",
        "    value = morpheme_tokenizer(value)\n",
        "  \n",
        "  for sequence in value:\n",
        "    sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "    sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "\n",
        "    if len(sequence_index) >= MAX_SEQUENCE:\n",
        "      sequence_index = sequence_index[:MAX_SEQUENCE -1] +[dictionary[END]]\n",
        "    else:\n",
        "      sequence_index += [dictionary[END]]\n",
        "\n",
        "    sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "    sequences_target_index.append(sequence_index)\n",
        "  \n",
        "  return np.asarray(sequences_target_index)"
      ]
    }
  ]
}